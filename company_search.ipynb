{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sentence_transformers import CrossEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = 'all-mpnet-base-v2' #all-mpnet-base-v2 #all-MiniLM-L6-v2\n",
    "embedding_model = SentenceTransformer(embedding_model_name)\n",
    "embedding_model.save(embedding_model_name)\n",
    "\n",
    "cross_encoder_model_name = 'cross-encoder/ms-marco-MiniLM-L-6-v2' # cross-encoder/stsb-roberta-base #cross-encoder/ms-marco-MiniLM-L-6-v2 \n",
    "cross_encoder_model = CrossEncoder(cross_encoder_model_name) \n",
    "cross_encoder_model.save(cross_encoder_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_filename = f'{embedding_model_name}_industry.pkl'\n",
    "col_to_embed = \"Company_name_industry\" #Company_name #Company_name_industry\n",
    "company_names_path = r\"all_company_names_final.csv\"\n",
    "df = pd.read_csv(company_names_path)\n",
    "\n",
    "if os.path.exists(pickle_filename ):\n",
    "    with open(pickle_filename , \"rb\") as fIn:\n",
    "       cache_data = pickle.load(fIn)\n",
    "       company_names = cache_data['company_names']\n",
    "       company_names_embeddings = cache_data['company_names_embeddings']\n",
    "       u3_nums = cache_data['u3_nums']\n",
    "\n",
    "else:\n",
    "\n",
    "    company_names = df['Company_name'].values.tolist()\n",
    "    company_names_for_embedding = df[col_to_embed].values.tolist()\n",
    "    u3_nums = df['u3_num'].values.tolist()\n",
    "    company_names_embeddings = embedding_model.encode(company_names_for_embedding, show_progress_bar=True, convert_to_tensor=True)\n",
    "\n",
    "    with open(pickle_filename, \"wb\") as fOut:\n",
    "        pickle.dump({'company_names': company_names, 'company_names_embeddings': company_names_embeddings, 'u3_nums': u3_nums}, \n",
    "                    fOut\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "main_dir = \"wsj\"\n",
    "ner_dir = \"gemma\"\n",
    "result_col_name = f\"company_found_{ner_dir}\"\n",
    "\n",
    "# outputs\n",
    "ner_mapped = f\"mapped_{ner_dir}\"\n",
    "\n",
    "if not os.path.exists(os.path.join(main_dir, ner_mapped)):\n",
    "    os.mkdir(os.path.join(main_dir, ner_mapped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_top_k_most_similar(query, top_k=100):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=False)\n",
    "    hits = util.semantic_search(query_embedding, company_names_embeddings, top_k=top_k)\n",
    "    hits = hits[0]  \n",
    "    candidates = [company_names[hit['corpus_id']] for hit in hits]\n",
    "    return hits, candidates\n",
    "\n",
    "def re_rank(query, hits, candidates):\n",
    "    sentence_pairs = [[query, candidate] for hit,candidate in zip(hits, candidates)]\n",
    "    ce_scores = cross_encoder_model.predict(sentence_pairs)\n",
    "    for ce_score, hit in zip(ce_scores, hits):\n",
    "        hit['cross-encoder_score'] = ce_score\n",
    "\n",
    "    # Sort by score, highest score at the top\n",
    "    hits = sorted(hits, key=lambda x: x['cross-encoder_score'], reverse=True)\n",
    "    candidates_reranked = [company_names[hit['corpus_id']] for hit in hits]\n",
    "    return hits, candidates_reranked\n",
    "\n",
    "def get_match(query, thresh_cross_encoder_score=0):\n",
    "\n",
    "    hits, candidates = return_top_k_most_similar(query, top_k=100)\n",
    "    hits, candidates_reranked = re_rank(query, hits, candidates)\n",
    "\n",
    "    company_found = None\n",
    "    for hit in hits:\n",
    "\n",
    "        candidate = company_names[hit['corpus_id']]\n",
    "        if float(hit['cross-encoder_score']) > thresh_cross_encoder_score :\n",
    "            company_found = candidate\n",
    "        else:\n",
    "            pass\n",
    "        break\n",
    "    return company_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_10.csv completed. Time elapsed:9.773066997528076\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(os.path.join(main_dir, ner_dir)):\n",
    "\n",
    "    filepath_save = os.path.join(os.path.join(main_dir, ner_mapped, filename))\n",
    "    if os.path.exists(filepath_save):\n",
    "        continue\n",
    "\n",
    "    df_batch = pd.read_csv(os.path.join(main_dir, ner_dir, filename))\n",
    "    vid_ids = df_batch['vid_id'].values.tolist()\n",
    "    company_names_found = df_batch[result_col_name].values.tolist()\n",
    "    queries = company_names_found\n",
    "\n",
    "    start_time = time.time()\n",
    "    company_names_mapped = []\n",
    "    u3_nums = []\n",
    "\n",
    "    for query in queries:\n",
    "\n",
    "        if pd.isnull(query):\n",
    "            company_names_mapped.append(np.nan)\n",
    "            u3_nums.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        match = get_match(query)\n",
    "        if match is None:\n",
    "            # remove industry and try to match again\n",
    "            query = re.sub(r\"(\\(.+\\))|\\[.+\\]\",'', query)\n",
    "            match = get_match(query) \n",
    "            if match is None:\n",
    "                company_names_mapped.append(np.nan)\n",
    "                u3_nums.append(np.nan)\n",
    "            else:\n",
    "                company_names_mapped.append(match)\n",
    "                u3_nums.append(df.loc[df['Company_name'] == match]['u3_num'].values.tolist()[0])\n",
    "        else:\n",
    "            company_names_mapped.append(match)\n",
    "            u3_nums.append(df.loc[df['Company_name'] == match]['u3_num'].values.tolist()[0])\n",
    "\n",
    "    df_batch_save = pd.DataFrame(zip(vid_ids, company_names_found, company_names_mapped, u3_nums), columns=['vid_id', result_col_name, 'Company_name_mapped', 'u3_num'])\n",
    "    df_batch_save.to_csv(os.path.join(os.path.join(main_dir, ner_mapped, filename)), index=False)\n",
    "\n",
    "    print(f\"{filename} completed. Time elapsed:{time.time() - start_time}\")\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the intermediate results, remove any possible duplicate entity in the same vid_id\n",
    "# save in a new file\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dict_approach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
